# BDH Training Configuration File
# Copyright Pathway Technology, Inc.

# Model Configuration
model:
  n_layer: 6
  n_embd: 256
  dropout: 0.1
  n_head: 4
  mlp_internal_dim_multiplier: 128
  vocab_size: 256

# Training Configuration
training:
  block_size: 512
  batch_size: 8
  max_iters: 1000000
  learning_rate: 3.0e-4
  weight_decay: 0.01
  gradient_accumulation_steps: 32  # Number of micro-batches to accumulate before optimizer step. Effective batch size = batch_size * gradient_accumulation_steps
  log_freq: 100
  test_freq: 1000
  checkpoint_freq: 10000
  runs_dir: "runs"
  resume_from_checkpoint: null #"runs/run_20251115_123456/checkpoint_1500.pt"  # Set to checkpoint path to resume
  debug: false
  compile_model: true  # Set to false for MPS compatibility (avoids threadgroup memory errors)

# TensorBoard Configuration
tensorboard:
  enabled: true  # Enable TensorBoard logging
  log_dir: "runs"  # Base directory for runs (contains checkpoints, logs, and TensorBoard events)
  log_gradients: true  # Log gradient histograms (can be expensive)
  log_weights: true  # Log weight histograms (can be expensive)
  log_scheduler_state: true  # Log scheduler internals like T_max, last_epoch (can be expensive)

# DataLoader Configuration
dataloader:
  num_workers: 4  # Number of worker processes for data loading
  pin_memory: true  # Speed up CPU->GPU transfer
  prefetch_factor: 2  # Prefetch 2 batches per worker

# Dataset Configuration
dataset:
  parquet_dir: "/mnt/scratch/fineweb/data/CC-MAIN-2025-26"
  max_cached_files: 5  # Keep only a few parquet files open
  val_split: 0.1  # Fraction of data files to use for validation (0.0-1.0)
  val_batches: 100  # Number of validation batches to evaluate (subsampling for efficiency)

# Low-Precision Training Configuration
# 
# This model uses nn.Parameter instead of nn.Linear layers, which limits some quantization options.
# However, we support multiple low-precision training modes:
# 
# 1. FP8/FP4 Native Compute (use_fp8: true) - NVIDIA Hopper/Blackwell GPUs:
#    - Uses hardware FP8 tensor cores for native 8-bit compute (Hopper H100, Blackwell)
#    - FP4 inference support on Blackwell GPUs
#    - Requires: pip install transformer-engine
#    - Best performance on H100/B100 GPUs with minimal accuracy loss
#    - Recommended for: Hopper/Blackwell GPUs
# 
# 2. 4-bit Optimizer (use_4bit_optimizer: true) - Any CUDA GPU:
#    - Uses AdamW8bit which stores optimizer states in 8-bit, reducing memory by ~75%
#    - Compatible with any model architecture
#    - Requires: pip install bitsandbytes
#    - Recommended for: Training with limited GPU memory
# 
# 3. Mixed Precision Training (via compute_dtype):
#    - Trains with lower precision (bfloat16/float16) to save memory and speed up training
#    - Works alongside other options
#
low_precision:
  use_fp8: false  # Set to true to use FP8 native compute (Hopper H100/Blackwell only)
  use_4bit: false  # Set to true to enable 4-bit training preparation
  use_4bit_optimizer: false  # Set to true to use 4-bit optimizer (AdamW8bit)
  quantization_type: "nf4"  # "nf4" or "fp4" - nf4 is better for normally distributed weights
  use_double_quant: false  # Use nested quantization for better compression
  compute_dtype: "bfloat16"  # Compute dtype: "bfloat16", "float16", or "float32"

# FP8 Configuration (Hopper/Blackwell GPUs only)
fp8:
  format: "hybrid"  # "e4m3" or "e5m2" or "hybrid" - hybrid uses both formats optimally
  amax_history_len: 1024  # History length for scaling factor computation
  amax_compute_algo: "max"  # "max" or "most_recent"

# Tokenizer Configuration
# 
# The tokenizer determines how text is converted to token IDs for the model.
# Different tokenizers offer different trade-offs:
# 
# 1. Byte-level (type: "byte") - Current default:
#    - vocab_size: 256 (all possible byte values)
#    - Language-agnostic, no dependencies
#    - Inefficient for long sequences
#    - Backward compatible with existing checkpoints
# 
# 2. TikToken (type: "tiktoken") - GPT-style tokenization:
#    - Requires: pip install tiktoken
#    - name: "gpt2" (vocab_size: 50257) or "cl100k_base" (vocab_size: 100277)
#    - 3-4x compression vs byte-level
#    - Fast Rust implementation
#    - Good for English text
# 
# 3. HuggingFace (type: "huggingface") - Flexible tokenization:
#    - Requires: pip install transformers
#    - name: any HuggingFace model (e.g., "gpt2", "bert-base-uncased")
#    - Supports many tokenizer types (BPE, WordPiece, etc.)
#    - Extensive model compatibility
# 
# 4. SentencePiece (type: "sentencepiece") - Custom tokenization:
#    - Requires: pip install sentencepiece
#    - name: path to .model file
#    - Language-agnostic, trainable
#    - Good for multilingual models
#
# NOTE: Changing tokenizer type requires retraining from scratch.
#       Model vocab_size will be auto-detected from tokenizer.
tokenizer:
  type: "tiktoken"  # Options: "byte", "tiktoken", "huggingface", "sentencepiece"
  name: "gpt2"  # For tiktoken: "gpt2" or "cl100k_base"; for HF: model name; for SP: path to .model
  pad_token_id: null  # Auto-detected from tokenizer if null
  eos_token_id: null  # Auto-detected from tokenizer if null
  bos_token_id: null  # Auto-detected from tokenizer if null

# Learning Rate Scheduler Configuration
# 
# Adaptive learning rate scheduling improves training convergence and performance.
# Different scheduler types offer different optimization strategies:
# 
# 1. None (type: "none") - Default, maintains constant learning rate:
#    - No learning rate adjustment
#    - Backward compatible with existing training
#    - Simplest approach, good baseline
# 
# 2. Cosine Annealing (type: "cosine") - Smooth decay with periodic restarts:
#    - Smoothly decreases LR following cosine curve
#    - T_max: REQUIRED - period of cosine annealing (typically max_iters - warmup_steps)
#    - min_lr: minimum learning rate at end of cycle
#    - Good for: Long training runs, fine-tuning
#    - Example: T_max=100000, min_lr=1e-6
# 
# 3. Linear Decay (type: "linear") - Gradual linear reduction:
#    - Linearly decreases LR from initial to min_lr
#    - min_lr: target minimum learning rate
#    - Good for: Simple decay strategy
# 
# 4. Exponential Decay (type: "exponential") - Multiplicative reduction:
#    - LR multiplied by gamma each step
#    - gamma: decay factor (e.g., 0.95 = 5% reduction per step)
#    - Good for: Aggressive decay
# 
# 5. Reduce on Plateau (type: "plateau") - Adaptive based on validation loss:
#    - Reduces LR when validation loss stops improving
#    - patience: number of evaluations without improvement before reducing
#    - factor: multiply LR by this when reducing (e.g., 0.1 = 10x reduction)
#    - threshold: minimum change to qualify as improvement
#    - Requires validation loss computation (controlled by val_batches)
#    - Good for: Adaptive training, avoiding overfitting
# 
# Warmup Phase (optional, all scheduler types):
#    - warmup_steps: number of steps to gradually increase LR from 0 to initial LR
#    - warmup_type: "linear" (gradual) or "constant" (immediate jump)
#    - Helps stabilize early training
#    - Recommended: 1000-5000 steps for large models
#
scheduler:
  type: "plateau"  # Options: "none", "cosine", "linear", "exponential", "plateau"
  warmup_steps: 0  # Number of warmup steps (absolute count, 0 = no warmup)
  warmup_type: "linear"  # Options: "linear", "constant"
  min_lr: 0.0  # Minimum learning rate for cosine/linear schedulers
  T_max: null  # REQUIRED for cosine: period of annealing (e.g., max_iters - warmup_steps)
  gamma: 0.95  # Exponential decay factor (LR *= gamma each step)
  patience: 10  # Plateau: evaluations without improvement before reducing LR
  factor: 0.1  # Plateau: factor to reduce LR by (new_lr = lr * factor)
  threshold: 1.0e-4  # Plateau: minimum change to count as improvement
